{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![ML_SWE-2.jpg](./Images/ML_SWE.jpg)\n",
    "\n",
    "# Modeling Approach\n",
    "\n",
    "* High-level overview of data-processing\n",
    "* Model Training\n",
    "* Preliminary Model Evaluation (iterate with model training)\n",
    "* Hindcast\n",
    "* Hindcast Evaluation (iterate through previous steps)\n",
    "\n",
    "\n",
    "## Machine Learning Approach\n",
    "\n",
    "Snow falls different accross the western US, and thus, was decided upon to develop a distributed ML SWE estimation model addressing the unique hydrometeorological variability observed in the modeling doamain.\n",
    "For example, the western U.S. contains snow climate classifications of coastal, coastal transitional, intermountain, and continental.\n",
    "The modeling framework addresses the heterogeneity in snow processes through the the division of the study area into 23 regional locations. \n",
    "Dividing the model into sub-regions allows for the separation of microclimates to reduce the influence of individual region dynamics on differing regions during model training. \n",
    "\n",
    "<img align = 'center' src=\"./Images/Distribution.jpg\" alt = 'drawing' width = '1000'/>\n",
    "\n",
    "## Data\n",
    "Machine learning models \"learn\" the relationships between independent and dependent variables through large amounts of data.\n",
    "Data sourced for the model consisted of geographic and topographic information from the Copernicus Digital Elevation Model (90-m DEM) and ground measurement data from the NRCS Snow Telemetry and Snow Course program (i.e., SNOTEL), as well as from the California Department of Water Resources California Data Exchange Center (CDEC). \n",
    "In total, geographic and weekly SWE observational data from 594 SNOTEL sites and 106 CDEC sites from 2013-2019 are collated.\n",
    "Weekly observations of the most recent date available at the same locations support near-real-time model inference. \n",
    "\n",
    "<img align = 'center' src=\"Images/Distribution_locations_number.jpg\" alt = 'drawing' width = '1000'/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Machine Learning Models\n",
    "There are many different types of machine learning models for differnt applications, such as classification, regression, and clustering.\n",
    "For the application of predicting 1-km gridded SWE, a regression model is the best approach.\n",
    "While there are many regression-based machine learning algorithms, we use [Light Gradient Boosted Models (LightGBM)](https://lightgbm.readthedocs.io/en/v3.3.2/) and [Multi-Layered Perceptron networks (MLP)](https://www.tensorflow.org/tutorials/keras/regression).\n",
    "Below is a brief description of each machine learning modeling methodlogy.\n",
    "\n",
    "\n",
    "### LightGBM\n",
    "Gradient boosted decision trees (GBDT) are a machine learning algorithm exhibiting impressive performance across various classification and regression applications.\n",
    "The algorithm generates a solution based on an ensemble of learning models, where weak learner trees, trained on the residuals of an initial strong learner, are iteratively added to the model to minimize the overall loss function (negative root-mean-squared-error) of the model via gradient descent of the individual weak learners. \n",
    "\n",
    "The LightGBM framework is an evolution of GBDT, and introduces Gradient-based One-Side Sampling (GOSS) to the boosting algorithm. \n",
    "GOSS focuses the model learning on trees with larger gradients and randomly drops learners with small gradients to provide a more efficient and more accurate gain estimation than with traditional gradient boosting. \n",
    "\n",
    "### MLP\n",
    "The MLP is a classical type of feedforward ANN, being successfully and frequently applied in environmental modeling applications.\n",
    "The MLP regression model estimates a target variable by learning a non-linear function to describe the target from an input vector of features.\n",
    "It performs learning via a back-propagation algorithm over a series of hidden layers containing interconnected nodes (neurons). \n",
    "The neurons connect bordering layers by a summation of weights and an activation function transforms model outputs to predicted values. \n",
    "The model calculates error and adjusts the weights to minimize the error during model training, supporting the use of \n",
    "MLPs to effectively describe a target variable with any function, continuous or discontinuous. \n",
    "\n",
    "\n",
    "[Next Chapter](./training.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c446eef832ec964573dc49f36fd16bdbed40cbfbefbf557bc2dc78d9e7968689"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
