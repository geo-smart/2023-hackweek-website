{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03363aa0",
   "metadata": {},
   "source": [
    "![ML_SWE-2.jpg](./Images/ML_SWE.jpg)\n",
    "\n",
    "# Prediction Simulation for Water Year 2019\n",
    "\n",
    "We will be testing and evaluating the performance of the model over the 2019 WY at select locations\n",
    "\n",
    "For example, the operational capacity of the model should begin at the beginning of the water year (WY week 1) with a previous SWE value of 0-in for each 1-km grid.\n",
    "Model evaluation could then assess the forecasting skill on a hold-out dataset of an entire year in which known neighboring values do not control predictions, but rather driving to-date predictions from the predicted previous SWE values. \n",
    "Due to the high model performance, future work will target the assessment of model skill with an operational motivation\n",
    "\n",
    "## Model Training/Testing influence on Model Results.\n",
    "\n",
    "The model training/testing partitioning methodology has a strong influence on model performance and the goal of model evaluation.\n",
    "The objective of the modeling effort was to examine the spatial extrapolation capacity of the model from selected monitoring stations to the overall region, best suited to a 75/25% training/testing split, respectively.\n",
    "While it is critical to address the strong serial correlation in SWE accumulation and melt throughout the season, the high correlation between weeks has the potential to inflate model skill when using a 75/25% training/testing split due to the previous SWE feature being known.\n",
    "An assessment of the operational capacity of the model is different than assessing the ability to extrapolate regional SWE from in-situ monitoring stations.\n",
    "For example, the operational capacity of the model should begin at the beginning of the water year (WY week 1) with a previous SWE value of 0-in for each 1-km grid.\n",
    "Model evaluation could then assess the forecasting skill on a hold-out dataset of an entire year in which known neighboring values do not control predictions, but rather driving to-date predictions from the predicted previous SWE values. \n",
    "Due to the high model performance, future work will target the assessment of model skill with an operational motivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13048ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import NSM_SCA #pip install progressbar\n",
    "import Hindcast_Initialization #pip install contextily\n",
    "\n",
    "#Set working directories\n",
    "cwd = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "datapath = os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0062b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_year = '2019'\n",
    "threshold = '20.0' #This threshold is standardized for now, to recalculate see Dr. Johnson\n",
    "Region_list = ['N_Sierras','S_Sierras_High', 'S_Sierras_Low']\n",
    "\n",
    "datelist = Hindcast_Initialization.Hindcast_Initialization(cwd, cwd, new_year, threshold, Region_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1216932",
   "metadata": {},
   "source": [
    "## Run your SSM in hindcast mode to evaluate operational capacity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386868b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run the model through all time (data acqusition already completed)\n",
    "for day in datelist:\n",
    "    print('Updating SWE predictions for ', day)\n",
    "    #connect interactive script to Wasatch Snow module\n",
    "    Snow = NSM_SCA.NSM_SCA(cwd, cwd, day, threshold=threshold, Regions = Region_list)\n",
    "    \n",
    "    #Go get SNOTEL observations - all data currently loaded, set to True to download\n",
    "    #Snow.Get_Monitoring_Data_Threaded(getdata = False)\n",
    "\n",
    "    #Initialize/Download the granules, all data preprocessed for the SSM activity, change to True to use the functions.\n",
    "    #Snow.initializeGranules(getdata = False)\n",
    "\n",
    "    #Process observations into Model prediction ready format,\n",
    "    #Snow.Data_Processing()\n",
    "\n",
    "    #Agument with SCA\n",
    "    #Snow.augmentPredictionDFs()\n",
    "\n",
    "    #Make predictions\n",
    "    Snow.SWE_Predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951e3a0-35a4-48e6-b137-c247a3f51048",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hindcast_Initialization.Snowgif(cwd, datelist, Region_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2290d9",
   "metadata": {},
   "source": [
    "## Model Hindcast complete\n",
    "\n",
    "Lets see how your model performs within SSWEET in the [Hindcast_Evaluation](./Hindcast_Evaluation.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
