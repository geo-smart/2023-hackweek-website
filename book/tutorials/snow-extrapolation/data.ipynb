{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "![ML_SWE-2.jpg](Images/ML_SWE.jpg)\n",
    "\n",
    "# Data Preparation and Feature Engineering\n",
    "Model data comes from the following sources National Resource Conservation Service (NRCS) Snow Telemetry (SNOTEL), California Data Exchange Center (CDEC), Copernicus 90-m DEM, and the NASA Airborne Snow Observatory (ASO).\n",
    "Feature engineering creates new features to capture the influences of the time of year, latitude, longitude, and elevation on SWE dynamics.\n",
    "Below is a generalized workflow for retrieving the data and engineering the following features for input into the machine learning models.\n",
    "\n",
    "|Feature id| Description|\n",
    " |:-----------: | :--------: |\n",
    " |WY Week | Numerical ID of the week of the water year|\n",
    " |Latitude | Center latitude of the training grid cell |\n",
    " |Longitude | Center longitude of the training grid cell|\n",
    " |Elevation | DEM elevation of the training grid cell |\n",
    " |Northness | Calculated northness of the training cell|\n",
    " |SNOTEL SWE | Current week's observed SWE from SNOTEL|\n",
    " |Prev SNOTEL SWE | Previous week's observed SWE from SNOTEL|\n",
    " |Delta SNOTEL SWE | Difference between Previous week's and current week's observed SWE from SNOTEL|\n",
    " |Previous SWE | Observed SWE from previous week|\n",
    " \n",
    " *Note, the Previous SWE feature is an observation for model training and testing but will be predicted when forecating.*\n",
    " \n",
    "## Data Processing/Retrievel Overview\n",
    "\n",
    "Data processing is likley the most tedious part of the model development pipeline and uses the elevation, slope, and aspect derived from DEM data through nearest-neighbor interpolation to produce values for all training, testing, and inference locations based on the four latitude and longitude corners of each grid cell.\n",
    "\n",
    "## Feature Engineering Overview\n",
    "Feature engineering consists of creating terrain (i.e., northness), temporal, and snow features.\n",
    "\n",
    "The calculation of the **northness metric** uses the slope and aspect values of each grid cell, using the embedded slope and aspect information within the metric to aid in ML model training and reducing the model dimensionality.\n",
    "\n",
    "<img align = 'center' src=\"Images/northness.jpg\" alt = 'drawing' width = '300'/>\n",
    "\n",
    "**Temporal features** support model training relative to seasonal snow accumulation and melt phases, consisting of a week-id as an integer from the beginning of the water year (WY) on October 1st.\n",
    "\n",
    "**Snow observation features** utilize in-situ station SWE observations, using the values observed from the current (Snotel/CDEC SWE) and the previous observation (Previous Snotel/CDEC SWE) as inputs.\n",
    "from current and previous observations, Delta SWE highlights the difference to capture the trend, either positive or negative, in SWE dynamics (i.e, melt or accumulation) with respect to each monitoring station ( $\\Delta$ SWE).\n",
    "\n",
    "**Grid cell previous observation features** capture the strong serial correlation of snow accumulation and melt on the SWE estimate, the model uses the SWE estimate of the previous week as a feature (Previous SWE).\n",
    "For model training and testing, the Previous SWE input is from NASA ASO datasets while the hindcast uses the previous model SWE estimate.\n",
    "\n",
    "### Below are simple data access and processing examples, please see the [National Snow Model Github](https://github.com/whitelightning450/National-Snow-Model) for complete examples.\n",
    "\n",
    "## Loading Provided SWE Observations from Snowcast Showdown\n",
    "\n",
    "The project supported the participation of the Snowcast Showdown, and while we shared useful code for SNOTEL and CDEC data retrievel (needed for forecasting), the information was provided by the United State Bureau of Reclaimation. \n",
    "We provide the supported modeling materials and provide the respective data processing and feature engineering steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install myst-nb pandas==1.4.3 h5py tqdm tables scikit-learn seaborn tensorflow progressbar contextily hydroeval geopandas==0.10.2  nbformat==5.7.0 pystac_client planetary_computer rioxarray matplotlib basemap numpy richdem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd #need to pip install 1.4.3\n",
    "import json \n",
    "from tqdm import tqdm #need to pip install\n",
    "import warnings; warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datapath ='/home/jovyan/shared-public/snow-extrapolation-web/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set up training DF with key metadata per site\n",
    "#All coordinates of 1 km polygon used to develop ave elevation, ave slope, ave aspect\n",
    "\n",
    "colnames = ['cell_id', 'Region', 'BR_Coord', 'UR_Coord', 'UL_Coord', 'BL_Coord']\n",
    "SWEdata = pd.DataFrame(columns = colnames)\n",
    "\n",
    "#Load training SWE data\n",
    "TrainSWE = pd.read_csv(f\"{datapath}/data/Prediction_Location_Observations.csv\")\n",
    "#drop na and put into modeling df format\n",
    "TrainSWE = TrainSWE.melt(id_vars=[\"cell_id\"]).dropna()\n",
    "\n",
    "#Load  SWE location data\n",
    "with open(f\"{datapath}/data/grid_cells.geojson\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "#load ground truth values(SNOTEL): training\n",
    "GM_Train = pd.read_csv(f\"{datapath}/data/ground_measures_train_features.csv\")\n",
    "#drop na and put into modeling df format\n",
    "GM_Train = GM_Train.melt(id_vars=[\"station_id\"]).dropna()\n",
    "\n",
    "#load ground truth values (SNOTEL): Testing\n",
    "GM_Test = pd.read_csv(f\"{datapath}/data/ground_measures_test_features.csv\")\n",
    "#drop na and put into modeling df format\n",
    "GM_Test = GM_Test.melt(id_vars=[\"station_id\"]).dropna()\n",
    "\n",
    "#load ground truth meta\n",
    "GM_Meta = pd.read_csv(f\"{datapath}/data/ground_measures_metadata.csv\")\n",
    "\n",
    "#merge training ground truth location metadata with snotel data\n",
    "GM_Train = GM_Meta.merge(GM_Train, how='inner', on='station_id')\n",
    "GM_Train = GM_Train.set_index('station_id')\n",
    "GM_Train.rename(columns={'name': 'location', 'latitude': 'Lat', 'longitude': 'Long', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "\n",
    "#merge testing ground truth location metadata with snotel data\n",
    "GM_Test = GM_Meta.merge(GM_Test, how='inner', on='station_id')\n",
    "GM_Test = GM_Test.set_index('station_id')\n",
    "GM_Test.rename(columns={'name': 'location', 'latitude': 'Lat', 'longitude': 'Long', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "#Make a SWE Grid location DF\n",
    "for i in tqdm(range(len(data[\"features\"]))):\n",
    "    properties = data[\"features\"][i][\"properties\"]\n",
    "    location = data[\"features\"][i][\"geometry\"]\n",
    "    DFdata = [properties [\"cell_id\"],  properties [\"region\"],location [\"coordinates\"][0][0] ,\n",
    "             location [\"coordinates\"][0][1], location [\"coordinates\"][0][2], location [\"coordinates\"][0][3] ]\n",
    "    df_length = len(SWEdata)\n",
    "    SWEdata.loc[df_length] = DFdata\n",
    "    \n",
    "#Make SWE location and observation DF\n",
    "#Training\n",
    "#merge site location metadata with observations\n",
    "TrainSWE = TrainSWE.merge(SWEdata, how='inner', on='cell_id')\n",
    "TrainSWE = TrainSWE.set_index('cell_id')\n",
    "TrainSWE.rename(columns={'variable': 'Date', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "#Make sure Date is in datetime data type\n",
    "TrainSWE['Date'] = pd.to_datetime(TrainSWE['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Sierra Nevada Mountains as a demonstration region for the tutorial\n",
    "For the tutorial, we select a subset of the region to reduce the computational burden on the end user which speeds up model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For model tutorial, selecting the Northern Rockies (UCOL)\n",
    "TrainSWE = TrainSWE[TrainSWE['Region'] =='sierras']\n",
    "SWEdata = SWEdata[SWEdata['Region'] =='sierras']\n",
    "GM_Train = GM_Train[GM_Train['state'] =='California']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(TrainSWE.head(5))\n",
    "print('There are: ', len(TrainSWE), ' training points in the Sierra dataset at ', len(TrainSWE.index.unique()), ' locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(GM_Train.head(5))\n",
    "print('There are: ', len(GM_Train.index.unique()), ' monitoring stations in the Sierra dataset providing ', len(GM_Train), ' observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get Lat Long information\n",
    "#Bottom right coord\n",
    "TrainSWE[['BR_Coord_Long','BR_Coord_Lat']] = pd.DataFrame(TrainSWE.BR_Coord.tolist(), index= TrainSWE.index)\n",
    "\n",
    "#Upper right coord\n",
    "TrainSWE[['UR_Coord_Long','UR_Coord_Lat']] = pd.DataFrame(TrainSWE.UR_Coord.tolist(), index= TrainSWE.index)\n",
    "\n",
    "#Upper left coord\n",
    "TrainSWE[['UL_Coord_Long','UL_Coord_Lat']] = pd.DataFrame(TrainSWE.UL_Coord.tolist(), index= TrainSWE.index)\n",
    "\n",
    "#Bottom Left coord\n",
    "TrainSWE[['BL_Coord_Long','BL_Coord_Lat']] = pd.DataFrame(TrainSWE.BL_Coord.tolist(), index= TrainSWE.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Copernicus Geospatial Data - 90 m DEM\n",
    "\n",
    " <img align = 'right' src=\"./Images/DEM.jpg\" alt = 'drawing' width = '600'/>\n",
    " \n",
    "We use the copernicus 90 m DEM hosted by [Microsoft](https://planetarycomputer.microsoft.com) to provide the geospatial information for each training and testing location.\n",
    "For each corner of each grid, we get the slope, elevation, and aspect, and calculate the average value to form as average 1-km geospatial information.\n",
    "We use the following code to access the DEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Develop a DF to get each site's geospatial information \n",
    "geocols = [ 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "       'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']\n",
    "\n",
    "\n",
    "Geospatial_df = TrainSWE.copy()\n",
    "Geospatial_df['rowid'] = Geospatial_df.index\n",
    "Geospatial_df = Geospatial_df.drop_duplicates(subset = 'rowid')\n",
    "Geospatial_df = pd.DataFrame(Geospatial_df[geocols])\n",
    "\n",
    "#for the tutorial, we will just use a few sites for demonstration purposes\n",
    "Geospatial_df = Geospatial_df.head(5)\n",
    "\n",
    "#Define the AOI around the cell locations from clockwise\n",
    "\n",
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            #lower left\n",
    "            [Geospatial_df['BL_Coord_Long'].min(), Geospatial_df['BL_Coord_Lat'].min()],\n",
    "            #upper left\n",
    "            [Geospatial_df['UL_Coord_Long'].min(), Geospatial_df['UL_Coord_Lat'].max()],\n",
    "            #upper right\n",
    "            [Geospatial_df['UR_Coord_Long'].max(), Geospatial_df['UR_Coord_Lat'].max()],\n",
    "            #lower right\n",
    "            [Geospatial_df['UR_Coord_Long'].max(), Geospatial_df['BR_Coord_Lat'].min()],\n",
    "            #lower left\n",
    "            [Geospatial_df['BL_Coord_Long'].min(), Geospatial_df['BL_Coord_Lat'].min()],\n",
    "        ]\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "area_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Geospatial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a connection to get 90m Copernicus Digital Elevation Model (DEM) data with the Planetary Computer STAC API\n",
    "from pystac_client import Client #need to pip install\n",
    "import planetary_computer #need to pip install\n",
    "\n",
    "client = Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    ignore_conformance=True,\n",
    ")\n",
    "\n",
    "\n",
    "search = client.search(\n",
    "    collections=[\"cop-dem-glo-90\"],\n",
    "    intersects=area_of_interest\n",
    ")\n",
    "\n",
    "tiles = list(search.get_items())\n",
    "\n",
    "#Make a DF to connect locations with the larger data tile, and then extract elevations\n",
    "regions = []\n",
    "\n",
    "for i in tqdm(range(0, len(tiles))):\n",
    "    row = [i, tiles[i].id]\n",
    "    regions.append(row)\n",
    "regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "regions = regions.set_index(regions['tileID'])\n",
    "del regions['tileID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes blocks can take some time as we are connecting the geospatial attributes of each corner of each grid, and then taking the average to get averaged grid geospatial attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added Long,Lat to get polygon points\n",
    "def GeoStat_func(i, Geospatial_df, regions, elev_L, slope_L, aspect_L, Long, Lat, tile):\n",
    "\n",
    "    # convert coordinate to raster value\n",
    "    lon = Geospatial_df.iloc[i][Long]\n",
    "    lat = Geospatial_df.iloc[i][Lat]\n",
    "\n",
    "    \n",
    "    \n",
    "    #connect point location to geotile\n",
    "    tileid = 'Copernicus_DSM_COG_30_N' + str(math.floor(lat)) + '_00_W'+str(math.ceil(abs(lon))) +'_00_DEM'\n",
    "    \n",
    "    indexid = regions.loc[tileid]['sliceID']\n",
    "    \n",
    "\n",
    "   #Assing region\n",
    "    signed_asset = planetary_computer.sign(tiles[indexid].assets[\"data\"])\n",
    "    #get elevation data in xarray object\n",
    "    elevation = rioxarray.open_rasterio(signed_asset.href)\n",
    "\n",
    "    #create copies to extract other geopysical information\n",
    "    #Create Duplicate DF's\n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "        \n",
    "    \n",
    "    #transform projection\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "    \n",
    "    #extract elevation values into numpy array\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "\n",
    "    #set tile geo to get slope and set at rdarray\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "    tilearray = rd.rdarray(tilearray, no_data = -9999)\n",
    "    tilearray.projection = 'EPSG:4326'\n",
    "    tilearray.geotransform = geo\n",
    "\n",
    "    #get slope, note that slope needs to be fixed, way too high\n",
    "    #get aspect value\n",
    "    slope_arr = rd.TerrainAttribute(tilearray, attrib='slope_degrees')\n",
    "    aspect_arr = rd.TerrainAttribute(tilearray, attrib='aspect')\n",
    "\n",
    "    #save slope and aspect information \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    \n",
    "    \n",
    "    #add point values to list\n",
    "    elev_L.append(elev)\n",
    "    slope_L.append(slop)\n",
    "    aspect_L.append(asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import rioxarray #need to pip install\n",
    "from pyproj import Transformer\n",
    "import numpy as np\n",
    "import richdem as rd #mamba install richdem -c conda-forge\n",
    "\n",
    "BLelev_L = []\n",
    "BLslope_L = []\n",
    "BLaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Geospatial_df, regions, BLelev_L, BLslope_L, BLaspect_L,\n",
    "                'BL_Coord_Long', 'BL_Coord_Lat', tiles) for i in tqdm(range(0, len(Geospatial_df)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['BL_Elevation_m'] = BLelev_L\n",
    "Geospatial_df['BL_slope_Deg'] = BLslope_L\n",
    "Geospatial_df['BLaspect_L'] = BLaspect_L\n",
    "\n",
    "\n",
    "ULelev_L = []\n",
    "ULslope_L = []\n",
    "ULaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Geospatial_df, regions, ULelev_L, ULslope_L, ULaspect_L,\n",
    "                'UL_Coord_Long', 'UL_Coord_Lat', tiles) for i in tqdm(range(0,len(Geospatial_df)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['UL_Elevation_m'] = ULelev_L\n",
    "Geospatial_df['UL_slope_Deg'] = ULslope_L\n",
    "Geospatial_df['ULaspect_L'] = ULaspect_L\n",
    "\n",
    "\n",
    "URelev_L = []\n",
    "URslope_L = []\n",
    "URaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Geospatial_df, regions, URelev_L, URslope_L, URaspect_L,\n",
    "                'UR_Coord_Long', 'UR_Coord_Lat', tiles) for i in tqdm(range(0,len(Geospatial_df)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['UR_Elevation_m'] = URelev_L\n",
    "Geospatial_df['UR_slope_Deg'] = URslope_L\n",
    "Geospatial_df['URaspect_L'] = URaspect_L\n",
    "\n",
    "\n",
    "BRelev_L = []\n",
    "BRslope_L = []\n",
    "BRaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "[GeoStat_func(i, Geospatial_df, regions, BRelev_L, BRslope_L, BRaspect_L,\n",
    "                'BR_Coord_Long', 'BR_Coord_Lat', tiles) for i in tqdm(range(0,len(Geospatial_df)))]\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['BR_Elevation_m'] = BRelev_L\n",
    "Geospatial_df['BR_slope_Deg'] = BRslope_L\n",
    "Geospatial_df['BRaspect_L'] = BRaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Geospatial_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Ground Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all unique Snotel sites\n",
    "Snotel = GM_Train.copy()\n",
    "Snotel = Snotel.reset_index()\n",
    "Snotel = Snotel.drop_duplicates(subset = ['station_id'])\n",
    "Snotel = Snotel.reset_index(drop = True) \n",
    "Snotel['Region'] = 'other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame and Engineer Features\n",
    "\n",
    "In this section, we begin building the DataFrames's, connect geospatial information, and process geospatial information via feature engineering.\n",
    "Essentially, the section connect geospatial information to training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean Geospatial data\n",
    "def mean_Geo(df, geo):\n",
    "    BL = 'BL'+geo\n",
    "    UL = 'UL'+geo\n",
    "    UR = 'UR'+geo\n",
    "    BR = 'BR'+geo\n",
    "    \n",
    "    df[geo] = (df[BL] + df[UL]+ df[UR] + df[BR]) /4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get geaspatial means\n",
    "geospatialcols = ['_Coord_Long', '_Coord_Lat', '_Elevation_m', '_slope_Deg' , 'aspect_L']\n",
    "\n",
    "#Training data\n",
    "[mean_Geo(Geospatial_df, i) for i in geospatialcols]\n",
    "\n",
    "#list of key geospatial component means\n",
    "geocol = ['_Coord_Long','_Coord_Lat','_Elevation_m','_slope_Deg','aspect_L']\n",
    "TrainGeo_df = Geospatial_df[geocol].copy()\n",
    "\n",
    "#adjust column names to be consistent with snotel\n",
    "TrainGeo_df = TrainGeo_df.rename( columns = {'_Coord_Long':'Long', '_Coord_Lat':'Lat', '_Elevation_m': 'elevation_m',\n",
    "                               '_slope_Deg':'slope_deg' , 'aspect_L': 'aspect'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainGeo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Modeling Domain into Sub-domains\n",
    "\n",
    " <img align = 'right' src=\"Images/CONUSsnow.jpg\" alt = 'drawing' width = '300'/>\n",
    " \n",
    "The Snowcast Showdown modeling domain covered the entire western US. \n",
    "Thus, because of differnces in snowpack characteristics (maritime, coastal transitional, intermountain, and continental) and regional climate patterns, we divided the original domain into 23 sub-domains. \n",
    "The figure is from Haegeli, P (2004), *Scale Analysis of avalanche activity on persistent snowpack weakness with respect to large-scale backcountry avalance forecasting.*\n",
    "For the the tutorial, we still need to perform the Region_id() function.\n",
    "However, we will focus on the Norther Colorado Rockies region, also refered to as the Upper Colorado River Basin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make Region identifier. The data already includes Region, but too many 'other' labels\n",
    "\n",
    "def Region_id(df):\n",
    "    \n",
    "    for i in tqdm(range(0, len(df))):\n",
    "        # Sierras\n",
    "        # Northern Sierras\n",
    "        if -122.5 <= df['Long'][i] <= -119 and 39 <= df['Lat'][i] <= 42:\n",
    "            loc = 'N_Sierras'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "        # Southern Sierras\n",
    "        if -122.5 <= df['Long'][i] <= -117 and 35 <= df['Lat'][i] <= 39:\n",
    "            loc = 'S_Sierras'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach a region id for each location\n",
    "TrainGeo_df['Region'] = 'other'\n",
    "Snotel['Region'] = 'other'\n",
    "GM_Train['Region'] = 'other'\n",
    "#Fix date variable\n",
    "GM_Train = GM_Train.rename(columns={'variable':'Date'})\n",
    "\n",
    "#Assign region to dataframes\n",
    "Region_id(TrainGeo_df)\n",
    "Region_id(Snotel)\n",
    "Region_id(GM_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainGeo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Snotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for slicing regions into regional DataFrames\n",
    "\n",
    "While this step is not needed for the tutorial, it supports the scaling to a larger doamain to ensure different regions are correctly classified. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset data by each region into dictionary\n",
    "RegionTrain = {name: TrainGeo_df.loc[TrainGeo_df['Region'] == name] for name in TrainGeo_df.Region.unique()}\n",
    "RegionSnotel  = {name: Snotel.loc[Snotel['Region'] == name] for name in Snotel.Region.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegionTrain['N_Sierras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegionTrain['S_Sierras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegionSnotel['N_Sierras'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegionSnotel['S_Sierras'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure no test locations classified as other\n",
    "print('Training') \n",
    "#look at region training sites\n",
    "for i in RegionTrain.keys():\n",
    "    print('There are', len(RegionTrain[i]), ' training locations in ', i)\n",
    "    \n",
    "print('         ') \n",
    "print('SNOTEL') \n",
    "#look at region training sites\n",
    "for i in RegionSnotel.keys():\n",
    "    print('There are', len(RegionSnotel[i]), ' Snotel locations in ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the NASA ASO and SNOTEL sites\n",
    "It is important for any modeling exercise to ensure your data is in the location that you expect.\n",
    "The GeoPlot() function plots the respective observations over a map of the Rocky mountains to visualize the modeling domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This plots the location of all df data points\n",
    "import matplotlib.pyplot as plt #need to pip install\n",
    "from mpl_toolkits.basemap import Basemap #pip install basemap\n",
    "import random\n",
    "\n",
    "\n",
    "def GeoPlot(df):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8, 6)\n",
    "\n",
    "    #merc also works for projection # Cylindrical Equal Area. https://matplotlib.org/basemap/api/basemap_api.html#module-mpl_toolkits.basemap\n",
    "\n",
    "    m = Basemap(projection='cea', \\\n",
    "                llcrnrlat=35, urcrnrlat=45, \\\n",
    "                llcrnrlon=-125, urcrnrlon=-115, \\\n",
    "                lat_ts=20, \\\n",
    "                resolution='c')\n",
    "\n",
    "    m.bluemarble(scale=2)   # full scale will be overkill\n",
    "    m.drawcoastlines(color='white', linewidth=0.2)  # add coastlines\n",
    "\n",
    "\n",
    "    # draw coastlines, meridians and parallels.\n",
    "\n",
    "    m.drawcountries()\n",
    "    m.drawstates()\n",
    "    m.drawparallels(np.arange(20,60,10),labels=[1,1,0,0])\n",
    "    m.drawmeridians(np.arange(-120,-90,10),labels=[0,0,0,1])\n",
    "\n",
    "\n",
    "    #Make unique color for each regions\n",
    "    number_of_colors = len(df.keys())\n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                 for i in range(number_of_colors)]\n",
    "\n",
    "    Location = list(df.keys())\n",
    "    colordict = {k: v for k, v in zip(Location, color)}\n",
    "\n",
    "\n",
    "    for i in df.keys():\n",
    "        x, y = m(np.array(df[i]['Long']), np.array(df[i]['Lat'])) \n",
    "        m.scatter(x, y, 10, marker='o', color=colordict[i], label = str(i)) \n",
    "\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "\n",
    "    plt.title('Locations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeoPlot(RegionTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeoPlot(RegionSnotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Geospatial data to SWE observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function connects stationary geospatial information to observations\n",
    "def Geo_to_Data(geodf, SWE, id):\n",
    "    dfcols = ['Long','Lat','elevation_m','slope_deg','aspect','Date','SWE','Region']\n",
    "    try:\n",
    "        SWE.pop('Region')\n",
    "    except:\n",
    "        print(' ')\n",
    "    geodf.reset_index(inplace = True)\n",
    "    SWE.reset_index(inplace = True)\n",
    "    datadf = SWE.merge(geodf, how='inner', on=id)\n",
    "    datadf.set_index(id, inplace = True)\n",
    "    datadf=datadf[dfcols]\n",
    "\n",
    "    return datadf\n",
    "\n",
    "#Create a temporal attribute, week_num(), that reflect the week id of the water year, beginning October 1st\n",
    "def week_num(df):\n",
    "        #week of water year\n",
    "    weeklist = []\n",
    "\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        if df['Date'][i].month<11:\n",
    "            y = df['Date'][i].year-1\n",
    "        else:\n",
    "            y = df['Date'][i].year\n",
    "            \n",
    "        WY_start = pd.to_datetime(str(y)+'-10-01')\n",
    "        deltaday = df['Date'][i]-WY_start\n",
    "        deltaweek = round(deltaday.days/7)\n",
    "        weeklist.append(deltaweek)\n",
    "\n",
    "\n",
    "    df['WYWeek'] = weeklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Geospatial data to SWE observations\n",
    "Snotel.set_index('station_id', inplace = True)\n",
    "\n",
    "#Connect location geospatial attributes to observations\n",
    "Training = Geo_to_Data(TrainGeo_df, TrainSWE, 'cell_id')\n",
    "\n",
    "# get snotel station id, region, slope, and aspect to merge with obervations\n",
    "GM_Snotel_train = GM_Train.copy()\n",
    "#Make Date in datetime dtype\n",
    "Training['Date'] = pd.to_datetime(Training['Date'])\n",
    "GM_Snotel_train['Date'] = pd.to_datetime(GM_Snotel_train['Date'])\n",
    "\n",
    "#add week number to observations\n",
    "week_num(Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_Snotel_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect observations to regional data\n",
    "#subset data by each region into dictionary\n",
    "RegionTrain = {name: Training.loc[Training['Region'] == name] for name in Training.Region.unique()}\n",
    "RegionSnotel_Train  = {name: GM_Snotel_train.loc[GM_Snotel_train['Region'] == name] for name in GM_Snotel_train.Region.unique()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RegionSnotel_Train['N_Sierras']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Make the Northness Feature\n",
    "\n",
    " <img align = 'center' src=\"Images/northness.jpg\" alt = 'drawing' width = '300'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function defines northness: :  sine(Slope) * cosine(Aspect). this gives you a northness range of -1 to 1.\n",
    "#Note you'll need to first convert to radians. \n",
    "#Some additional if else statements to get around sites with low obervations\n",
    "def northness(df):    \n",
    "    \n",
    "    if len(df) == 8: #This removes single value observations, need to go over and remove these locations from training too\n",
    "        #Determine northness for site\n",
    "        #convert to radians\n",
    "        df = pd.DataFrame(df).T\n",
    "        \n",
    "        df['aspect_rad'] = df['aspect']*0.0174533\n",
    "        df['slope_rad'] = df['slope_deg']*0.0174533\n",
    "        \n",
    "        df['northness'] = -9999\n",
    "        for i in range(0, len(df)):\n",
    "            df['northness'].iloc[i] = math.sin(df['slope_rad'].iloc[i])*math.cos(df['aspect_rad'].iloc[i])\n",
    "\n",
    "        #remove slope and aspects to clean df up\n",
    "        df = df.drop(columns = ['aspect', 'slope_deg', 'aspect_rad', 'slope_rad', 'Region'])\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    else:\n",
    "         #convert to radians\n",
    "        df['aspect_rad'] = df['aspect']*0.0174533\n",
    "        df['slope_rad'] = df['slope_deg']*0.0174533\n",
    "        \n",
    "        df['northness'] = -9999\n",
    "        for i in range(0, len(df)):\n",
    "            df['northness'].iloc[i] = math.sin(df['slope_rad'].iloc[i])*math.cos(df['aspect_rad'].iloc[i])\n",
    "\n",
    "        \n",
    "         #remove slope and aspects to clean df up\n",
    "        df = df.drop(columns = ['aspect', 'slope_deg', 'aspect_rad', 'slope_rad', 'Region'])\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make northness feature and delete regions, slope, aspect features for each training and testing cell\n",
    "for i in tqdm(RegionTrain):\n",
    "    RegionTrain[i] = northness(RegionTrain[i])\n",
    "    \n",
    "#Make dictionary in Regions dict for each region's dictionary of Snotel sites\n",
    "Regions = list(RegionTrain.keys()).copy()\n",
    "\n",
    "#Make northness for all Snotel observations\n",
    "for i in tqdm(Regions):\n",
    "    \n",
    "    snotel = i+'_Snotel'\n",
    "    RegionTrain[snotel] = {site: RegionSnotel_Train[i].loc[site] for site in RegionSnotel_Train[i].index.unique()}\n",
    "    \n",
    "    #get training and testing sites that are the same\n",
    "    train = RegionTrain[snotel].keys()\n",
    "    \n",
    "    #make Northing metric\n",
    "    for j in tqdm(train):\n",
    "  \n",
    "    #remove items we do not need\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].drop(columns = ['Long', 'Lat'])\n",
    "    #make date index\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].set_index('Date')\n",
    "        \n",
    "    #rename columns to represent site info\n",
    "        colnames = RegionTrain[snotel][j].columns\n",
    "        sitecolnames = [x +'_'+ j for x in colnames]\n",
    "        names = dict(zip(colnames, sitecolnames))\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].rename(columns = names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(Regions):\n",
    "    \n",
    "    snotel = i+'_Snotel'\n",
    "    RegionTrain[snotel] = {site: RegionSnotel_Train[i].loc[site] for site in RegionSnotel_Train[i].index.unique()}\n",
    "    \n",
    "    #get training and testing sites that are the same\n",
    "    train = RegionTrain[snotel].keys()\n",
    "\n",
    "    \n",
    "    #get obs\n",
    "    for j in tqdm(train):\n",
    "   \n",
    "    #remove items we do not need\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].drop(columns = ['Long', 'Lat', 'elevation_m', 'location', 'state' , 'Region'])\n",
    "    #make date index\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].set_index('Date')\n",
    "        \n",
    "    #rename columns to represent site info\n",
    "        colnames = RegionTrain[snotel][j].columns\n",
    "        sitecolnames = [x +'_'+ j for x in colnames]\n",
    "        names = dict(zip(colnames, sitecolnames))\n",
    "        RegionTrain[snotel][j] = RegionTrain[snotel][j].rename(columns = names)\n",
    "    \n",
    "    #Remove unused columns\n",
    "    columns = list(RegionTrain[snotel].keys()).copy()\n",
    "    for col in columns:\n",
    "        if len(RegionTrain[snotel][col].columns) >4:\n",
    "            del RegionTrain[snotel][col]\n",
    "\n",
    "            \n",
    "#make a df for training each region, \n",
    "for R in tqdm(Regions):\n",
    "    snotels = R+'_Snotel'\n",
    "    RegionTrain[R] = RegionTrain[R].reset_index()\n",
    "    RegionTrain[R] = RegionTrain[R].set_index('Date')\n",
    "      \n",
    "    for S in RegionTrain[snotels]:\n",
    "        RegionTrain[R]= pd.concat([RegionTrain[R], RegionTrain[snotels][S].reindex(RegionTrain[R].index)], axis=1)\n",
    "    \n",
    "    RegionTrain[R] = RegionTrain[R].fillna(-9999)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegionTrain['N_Sierras'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Previous Week's SNOTEL SWE\n",
    "\n",
    "We use in-situ station SWE observations as features, using the values observed from the current (Snotel/CDEC SWE) and the previous week (Previous Snotel/CDEC SWE) as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prev_SWE_Snotel_Dict(DF, region):\n",
    "    print(region)\n",
    "    \n",
    "    regionsnotel = region+'_Snotel'\n",
    "    \n",
    "    sites = DF[regionsnotel].keys()\n",
    "    \n",
    "    #week delta  \n",
    "    weekdelta = pd.Timedelta(7, \"d\")\n",
    "    \n",
    "    for i in tqdm(sites):\n",
    "        #print(i)\n",
    "        prevSWE = 'Prev_SWE_' + i\n",
    "        SWE = 'SWE_'+i\n",
    "        \n",
    "        DF[regionsnotel][i][prevSWE] = -9999.99\n",
    "        \n",
    "        #need to find the number of columns for ifelse\n",
    "        dfcols = len(DF[regionsnotel][i].columns)\n",
    "    \n",
    "\n",
    "        #if only one observation need to fix\n",
    "        if len(DF[regionsnotel][i]) == 1:\n",
    "            DF[regionsnotel][i] = DF[regionsnotel][i].T\n",
    "\n",
    "        for cell in range(1,len(DF[regionsnotel][i])):\n",
    "\n",
    "            if DF[regionsnotel][i].index[cell] - DF[regionsnotel][i].index[cell-1] == weekdelta:     \n",
    "                DF[regionsnotel][i][prevSWE][cell] = DF[regionsnotel][i][SWE][cell-1]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in Regions:\n",
    "    Prev_SWE_Snotel_Dict(RegionTrain, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegionTrain['N_Sierras_Snotel']['CDEC:ADM'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Delta SNOTEL SWE\n",
    "\n",
    "Using the observations from the current and previous week, we calculate the difference to capture the trend, either positive or negative, in SWE dynamics (i.e, melt or accumulation) with respect to each monitoring station ( $\\Delta$ SWE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delta_SWE_Snotel_Dict(DF, region):\n",
    "    # print(region)\n",
    "    \n",
    "    regionsnotel = region+'_Snotel'\n",
    "    \n",
    "    sites = DF[regionsnotel].keys()\n",
    "    \n",
    "    for i in tqdm(sites):\n",
    "      #  print(i)\n",
    "        prevSWE = 'Prev_SWE_' + i\n",
    "        SWE = 'SWE_'+i\n",
    "        Delta_SWE = 'Delta_'+SWE\n",
    "        \n",
    "        DF[regionsnotel][i][Delta_SWE] = DF[regionsnotel][i][SWE] - DF[regionsnotel][i][prevSWE]\n",
    "        DF[regionsnotel][i].loc[DF[regionsnotel][i][Delta_SWE]>150, Delta_SWE] =-9999.99\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Delta SWE feature\n",
    "for i in Regions:\n",
    "    Delta_SWE_Snotel_Dict(RegionTrain, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegionTrain['N_Sierras_Snotel']['CDEC:ADM'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Snotel Observations to NASA ASO\n",
    "Connect dataframe of NASA ASO with snotel observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a df for training each region, \n",
    "for R in tqdm(Regions):\n",
    "    snotels = R+'_Snotel'\n",
    "    RegionTrain[R] = RegionTrain[R].reset_index()\n",
    "    RegionTrain[R] = RegionTrain[R].set_index('Date')\n",
    "    \n",
    "    for S in RegionTrain[snotels]:\n",
    "        RegionTrain[R]= pd.concat([RegionTrain[R], RegionTrain[snotels][S].reindex(RegionTrain[R].index)], axis=1)\n",
    "    \n",
    "    RegionTrain[R] = RegionTrain[R].fillna(-9999)\n",
    "\n",
    "#Remove unnecessary features\n",
    "for region in Regions:\n",
    "    RegionTrain[region] = RegionTrain[region].drop( RegionTrain[region].filter(regex='elevation_m_').columns, axis=1)\n",
    "    RegionTrain[region] = RegionTrain[region].drop( RegionTrain[region].filter(regex='northness_').columns, axis=1)\n",
    "    RegionTrain[region] = RegionTrain[region].T.drop_duplicates().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Site's Previous SWE \n",
    "\n",
    "Because of the serial correlation of snow accumulation and melt on the current timesteps SWE prediction, the model uses the SWE estimate of the previous week as a feature (Previous SWE).\n",
    "For model training and testing, the Previous SWE input is from NASA ASO datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Prev_SWE(df, region):    \n",
    "    print(region)\n",
    "    \n",
    "    df = df[region].reset_index()\n",
    "    df = df.set_index('cell_id')\n",
    "    \n",
    "    #week delta  \n",
    "    weekdelta = pd.Timedelta(7, \"d\")\n",
    "\n",
    "    #set up column for previous weeks SWE\n",
    "    df['prev_SWE'] = -9999.99\n",
    "    \n",
    "    #need to find the number of columns for ifelse\n",
    "    dfcols = len(df.columns)\n",
    "    \n",
    "    #Run through each uniqe site/cell id to calculate previous weeks SWE and add to a new dataframe\n",
    "    new_df = pd.DataFrame(columns = df.columns)\n",
    "    \n",
    "    #find unique sites\n",
    "    sites = df.index.unique()\n",
    " \n",
    "    for i in tqdm(sites):\n",
    "        site = df.loc[i].copy()\n",
    "\n",
    "        #if only one observation need to fix\n",
    "        if site.shape == (dfcols,):# and len(site) < 162:\n",
    "            site = site.to_frame().T\n",
    "\n",
    "        for cell in range(1,len(site)):\n",
    "            if site['Date'][cell] - site['Date'][cell-1] == weekdelta:     \n",
    "                site['prev_SWE'][cell] = site['SWE'][cell-1]\n",
    "        dflist = [new_df, site]\n",
    "        new_df = pd.concat(dflist)\n",
    "    new_df = new_df.fillna(-9999)\n",
    "    \n",
    "    #Put Prev_SWE next to SWE to confirm operations\n",
    "    prev_SWE = new_df['prev_SWE'].copy()\n",
    "    del new_df['prev_SWE']\n",
    "    new_df.insert(loc = 5,\n",
    "          column = 'prev_SWE',\n",
    "          value = prev_SWE)\n",
    "    \n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the previous SWE function and save the dataframe, you will now be ready to train the model.\n",
    "training_df = {}\n",
    "for region in Regions:\n",
    "    training_df[region] = Prev_SWE(RegionTrain, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['N_Sierras'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['S_Sierras'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next [Chapter](./training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackweek",
   "language": "python",
   "name": "hackweek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c446eef832ec964573dc49f36fd16bdbed40cbfbefbf557bc2dc78d9e7968689"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
